{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset , Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, LongT5ForConditionalGeneration, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_file_path = \"../../data/train_research.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model ={}\n",
    "dict_model[\"model_ckpt_longt5_globalbase\"] = \"google/long-t5-tglobal-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Data\n",
    "\n",
    "def getDataNRows(filename,cleanCol, n=100):\n",
    "    \"\"\"\n",
    "    Gets the data in dataset format for n rows of  csv file\n",
    "\n",
    "    filename: full path to the csv file\n",
    "\n",
    "    n: Number of Rows needed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    d = pd.read_csv(filename)\n",
    "    d = d.head(n)\n",
    "    dataset = Dataset.from_pandas(d)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#Getting the data ready\n",
    "data = getDataNRows(train_file_path,\"sections\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sections', 'abstract', 'summary'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sections', 'abstract', 'summary'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sections', 'abstract', 'summary'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data\n",
    "ds = data.train_test_split(test_size=0.2)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "#Loading the Model\n",
    "def get_tokenzier_model(model_ckpt):\n",
    "\n",
    "    \"\"\"\n",
    "    returns the tokenizer and the model for a specific model checkpoint\n",
    "\n",
    "    model_ckpt: model checkpoint name\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "    if model_ckpt == \"google/long-t5-tglobal-base\":\n",
    "      model = LongT5ForConditionalGeneration.from_pretrained(model_ckpt, torch_dtype=torch.bfloat16)\n",
    "      print(\"success\")\n",
    "    else:\n",
    "      model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_tokenzier_model(dict_model[\"model_ckpt_longt5_globalbase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model params :247587456 \n",
      "all model params : 247587456 \n",
      "Percentage of params trainale 100.0 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def print_number_model_params(model):\n",
    "    trainable = 0\n",
    "    all_params =0 \n",
    "    for _, param in model.named_parameters():\n",
    "        all_params +=param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable += param.numel()\n",
    "    return f\"\"\"trainable model params :{trainable} \\nall model params : {all_params} \\nPercentage of params trainale {(trainable/all_params)*100} %\"\"\"\n",
    "\n",
    "print(print_number_model_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Trying\n",
    "# prefix = \"summarize: \"\n",
    "\n",
    "# def preprocess_function1(examples):\n",
    "#     inputs = [prefix + doc for doc in examples[\"sections\"]]\n",
    "#     examples[\"input_ids\"]  = tokenizer(inputs,  max_length=1024,padding=True,truncation=True,return_tensors=\"pt\").input_ids\n",
    "#     examples[\"labels\"] = tokenizer(examples[\"summary\"],  max_length=200,padding=True,truncation=True,return_tensors=\"pt\").input_ids\n",
    "#     return examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_ds = ds.map(preprocess_function1, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9828ce71e96471db6cf1606c00ae701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e094b404de3640d3ad99a273b40f06a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenizing the input\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"sections\"]]\n",
    "    # model_inputs = tokenizer(inputs, max_length=16384, truncation=True)\n",
    "    model_inputs = tokenizer(inputs, max_length=4000, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=200, truncation=False) #'sections', 'abstract', 'summary'\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "    \n",
    "\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sections', 'abstract', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sections', 'abstract', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remvoing all the other cols and leaving only\n",
    "tokenized_ds = tokenized_ds.remove_columns(column_names=[\"sections\",\"abstract\",\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape {tokenized_ds['train'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Collation\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=dict_model[\"model_ckpt_longt5_globalbase\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "#Evaluation metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    To compute metrics while training\n",
    "\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setting UP LORA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trting T5 base model\n",
    "\n",
    "# from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# checkpoint = \"t5-small\"\n",
    "# model1 = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config=LoraConfig(\n",
    "r=32, # Rank\n",
    "lora_alpha=32,\n",
    "target_modules=[\"q\",\"v\"],\n",
    "lora_dropout=0.05,\n",
    "bias=\"none\",\n",
    "task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model params :3538944 \n",
      "all model params : 251126400 \n",
      "Percentage of params trainale 1.409228181505409 %\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model,lora_config)\n",
    "\n",
    "print(print_number_model_params(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Adapeter\n",
    "\n",
    "import time\n",
    "output_dir  = f'../../artifacts/lora_training_{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir  = output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=2,\n",
    "    max_steps=1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "peft_trainer=Trainer(\n",
    "\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aa0713e572409db8503ff327423ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d04620480c42cd84a81c4f4c7d7297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m peft_trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      3\u001b[0m peft_model_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../../artifacts/lora_training_checkpoints\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(\u001b[39mint\u001b[39m(time\u001b[39m.\u001b[39mtime()))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m peft_trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(peft_model_path)\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\accelerate\\utils\\memory.py:132\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo executable batch size found, reached zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     \u001b[39mreturn\u001b[39;00m function(batch_size, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    133\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\trainer.py:2021\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2018\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 2021\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   2023\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[0;32m   2024\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   2025\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\trainer.py:2287\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2285\u001b[0m             metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2286\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2287\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   2288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\trainer.py:2993\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2990\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2992\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2993\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2994\u001b[0m     eval_dataloader,\n\u001b[0;32m   2995\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2996\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2997\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2998\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2999\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   3000\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   3001\u001b[0m )\n\u001b[0;32m   3003\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   3004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\trainer.py:3281\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3277\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3278\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[0;32m   3279\u001b[0m         )\n\u001b[0;32m   3280\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3281\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[0;32m   3282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3283\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[100], line 13\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mTo compute metrics while training\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m predictions, labels \u001b[39m=\u001b[39m eval_pred\n\u001b[1;32m---> 13\u001b[0m decoded_preds \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_decode(predictions, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     14\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(labels \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m, labels, tokenizer\u001b[39m.\u001b[39mpad_token_id)\n\u001b[0;32m     15\u001b[0m decoded_labels \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(labels, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3446\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   3423\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   3424\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3427\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3428\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   3429\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3430\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3431\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3444\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3446\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   3447\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   3448\u001b[0m             seq,\n\u001b[0;32m   3449\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3450\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3451\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3452\u001b[0m         )\n\u001b[0;32m   3453\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   3454\u001b[0m     ]\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3447\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   3423\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   3424\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3427\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3428\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   3429\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3430\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3431\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3444\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3446\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m-> 3447\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   3448\u001b[0m             seq,\n\u001b[0;32m   3449\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3450\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3451\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3452\u001b[0m         )\n\u001b[0;32m   3453\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   3454\u001b[0m     ]\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3484\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3463\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3464\u001b[0m \u001b[39mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m   3465\u001b[0m \u001b[39mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3481\u001b[0m \u001b[39m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m   3482\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3483\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m-> 3484\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m   3486\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[0;32m   3487\u001b[0m     token_ids\u001b[39m=\u001b[39mtoken_ids,\n\u001b[0;32m   3488\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3489\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3490\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3491\u001b[0m )\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:197\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(obj)\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    196\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mnumber)):  \u001b[39m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mtolist()\n\u001b[0;32m    198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = f\"../../artifacts/lora_training_checkpoints{str(int(time.time()))}\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging both the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "#We already have the original model and the tokenizer, we just need to load\n",
    "#the Peft model and merge it with base\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,'../../artifacts/lora_training_checkpoints1689416691/',torch_dtype=torch.bfloat16,is_trainable=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model params :0 \n",
      "all model params : 251126400 \n",
      "Percentage of params trainale 0.0 %\n"
     ]
    }
   ],
   "source": [
    "print(print_number_model_params(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "section  = ds['test'][index]['sections']\n",
    "original_summary = ds['test'][index]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "promopt = f\"\"\"\n",
    "\n",
    "summarize: \n",
    "\n",
    "{section}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_ids= tokenizer(promopt,return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = peft_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=4))\n",
    "output_text = tokenizer.decode(output[0],skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original summmary\n",
      " Deep learning in in vitro fertilization is currently being evaluated in the development of assistive tools for the determination of transfer order and implantation potential using time-lapse data collected through expensive imaging hardware . Assistive tools and algorithms that can work with static images , however , can help in improving the access to care by enabling their use with images acquired from traditional microscopes that are available to virtually all fertility centers . Here , we evaluated the use of a deep convolutional neural network ( CNN ) , trained using single timepoint images of embryos collected at 113 hr post-insemination , in embryo selection amongst 97 clinical patient cohorts ( 742 embryos ) and observed an accuracy of 90% in choosing the highest quality embryo available . Furthermore , a CNN trained to assess an embryo’s implantation potential directly using a set of 97 euploid embryos capable of implantation outperformed 15 trained embryologists ( 75 . 26% vs . 67 . 35% , p<0 . 0001 ) from five different fertility centers .\n",
      "------------------------------------------------------------------------------------------\n",
      "generated summary: Although multiple factors such as maternal age, medical diagnosis, gamete and embryo quality, and endometrium receptivity determine the success of ART cycles, the challenge of non-invasive selection of the highest available quality from a patient’s cohort of embryos ( top-quality embryo ) for transfer remains as one of the most important factors in achieving successful ART outcomes ( Vaegter et al. Emulating the skill of highly trained embryologists in efficient embryo assessment in a fully automated system is a major challenge in all of the previous work done in computer-aided assessments of embryos due to focus on measuring specific expert-defined parameters such as zona pellucida thickness variation, number of blastomeres, degree of cell symmetry and cytoplasmic fragmentation, etc. The use of deep-learning in IVF has also been explored; however \n"
     ]
    }
   ],
   "source": [
    "dash = \"------------------------------------------------------------------------------------------\"\n",
    "print(f\"original summmary\\n {original_summary}\")\n",
    "print(dash)\n",
    "print(f\"generated summary: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original summmary\n",
      " Deep learning in in vitro fertilization is currently being evaluated in the development of assistive tools for the determination of transfer order and implantation potential using time-lapse data collected through expensive imaging hardware . Assistive tools and algorithms that can work with static images , however , can help in improving the access to care by enabling their use with images acquired from traditional microscopes that are available to virtually all fertility centers . Here , we evaluated the use of a deep convolutional neural network ( CNN ) , trained using single timepoint images of embryos collected at 113 hr post-insemination , in embryo selection amongst 97 clinical patient cohorts ( 742 embryos ) and observed an accuracy of 90% in choosing the highest quality embryo available . Furthermore , a CNN trained to assess an embryo’s implantation potential directly using a set of 97 euploid embryos capable of implantation outperformed 15 trained embryologists ( 75 . 26% vs . 67 . 35% , p<0 . 0001 ) from five different fertility centers .\n",
      "------------------------------------------------------------------------------------------\n",
      "generated summary: Although multiple factors such as maternal age, medical diagnosis, gamete and embryo quality, and endometrium receptivity determine the success of ART cycles, the challenge of non-invasive selection of the highest available quality from a patient’s cohort of embryos ( top-quality embryo ) for transfer remains as one of the most important factors in achieving successful ART outcomes ( Vaegter et al. Emulating the skill of highly trained embryologists in efficient embryo assessment in a fully automated system is a major challenge in all of the previous work done in computer-aided assessments of embryos due to focus on measuring specific expert-defined parameters such as zona pellucida thickness variation, number of blastomeres, degree of cell symmetry and cytoplasmic fragmentation, etc. The use of deep-learning in IVF has also been explored; however \n"
     ]
    }
   ],
   "source": [
    "dash = \"------------------------------------------------------------------------------------------\"\n",
    "print(f\"original summmary\\n {original_summary}\")\n",
    "print(dash)\n",
    "print(f\"generated summary: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning in in vitro fertilization is currently being evaluated in the development of assistive tools for the determination of transfer order and implantation potential using time-lapse data collected through expensive imaging hardware . Assistive tools and algorithms that can work with static images , however , can help in improving the access to care by enabling their use with images acquired from traditional microscopes that are available to virtually all fertility centers . Here , we evaluated the use of a deep convolutional neural network ( CNN ) , trained using single timepoint images of embryos collected at 113 hr post-insemination , in embryo selection amongst 97 clinical patient cohorts ( 742 embryos ) and observed an accuracy of 90% in choosing the highest quality embryo available . Furthermore , a CNN trained to assess an embryo’s implantation potential directly using a set of 97 euploid embryos capable of implantation outperformed 15 trained embryologists ( 75 . 26% vs . 67 . 35% , p<0 . 0001 ) from five different fertility centers .'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m rouge \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m results \u001b[39m=\u001b[39m rouge\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m      4\u001b[0m     pred \u001b[39m=\u001b[39;49m output_text,\n\u001b[0;32m      5\u001b[0m     references\u001b[39m=\u001b[39;49moriginal_summary,\n\u001b[0;32m      6\u001b[0m     use_aggregator\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      7\u001b[0m     use_stemmer\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\evaluate\\module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_batch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[0;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\evaluate\\module.py:480\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m batch \u001b[39m=\u001b[39m {input_name: batch[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_feature_from_batch(batch)\n\u001b[0;32m    481\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_writer()\n\u001b[0;32m    482\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\evaluate\\module.py:551\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\n\u001b[0;32m    550\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([(k, v[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()])\n\u001b[0;32m    552\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[1;32md:\\UCDPROJECT\\TLDR\\.venv\\lib\\site-packages\\evaluate\\module.py:551\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\n\u001b[0;32m    550\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([(k, v[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()])\n\u001b[0;32m    552\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_feature_from_example(example)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "results = rouge.compute(\n",
    "    pred = output_text,\n",
    "    references=original_summary,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
